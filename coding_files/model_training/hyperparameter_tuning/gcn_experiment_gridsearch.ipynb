{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node classification with Graph Convolutional Network (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mygene\n",
    "import h5py\n",
    "import pickle\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.layer import GCN\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph import datasets\n",
    "from stellargraph.mapper import (\n",
    "    CorruptedGenerator,\n",
    "    FullBatchNodeGenerator,\n",
    "    GraphSAGENodeGenerator,\n",
    "    HinSAGENodeGenerator,\n",
    "    ClusterNodeGenerator,\n",
    ")\n",
    "\n",
    "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE\n",
    "from stellargraph.utils import plot_history\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import average_precision_score\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = pd.read_csv('C:/Users/renan/Desktop/UFRGS/GNN/data/data_final_v2/HPRD_network.tsv', sep='\\t')\n",
    "\n",
    "features = pd.read_csv('C:/Users/renan/Desktop/UFRGS/GNN/data/data_final_v2/HPRD_features_complete.tsv', sep='\\t', index_col='gene')\n",
    "\n",
    "labels = pd.read_csv('C:/Users/renan/Desktop/UFRGS/GNN/data/data_final_v2/HPRD_labels_semisupervised.tsv', sep='\\t', index_col='gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutations 0:48\n",
    "# CNA 16:64\n",
    "# DNA Methylation 0:32 e 16:48\n",
    "# Gene Expression 0:16 e 16:48\n",
    "\n",
    "\n",
    "#features.drop(features.iloc[:, 0:16], inplace = True, axis = 1)\n",
    "#features.drop(features.iloc[:, 16:48], inplace = True, axis = 1)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar as labels boleanas em 0/1\n",
    "labels[\"label\"].replace({False: 0, True: 1}, inplace=True)\n",
    "\n",
    "# Transformar as labels vazias em -1\n",
    "labels[\"label\"] = labels.label.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 9438, Edges: 36844\n",
      "\n",
      " Node types:\n",
      "  default: [9438]\n",
      "    Features: float32 vector, length 68\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [36844]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "G = StellarGraph(edges=network, nodes=features)\n",
    "\n",
    "print(G.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>3772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       " 0.0   4873\n",
       "-1.0   3772\n",
       " 1.0    793"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_classes = labels['label']\n",
    "\n",
    "series_classes.value_counts(dropna = False).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  4532\n",
      "Test:  1134\n",
      "\n",
      "Total:  5666\n"
     ]
    }
   ],
   "source": [
    "# Dividindo dados em treino/validação/teste\n",
    "\n",
    "train_ratio = 0.80\n",
    "test_ratio = 0.20\n",
    "\n",
    "labeled_data = labels[labels['label'] != -1]\n",
    "labeled_data = labeled_data.sample(frac=1)\n",
    "\n",
    "# Aqui aplica-se então 20% do tamanho total da rede e o restante para treino\n",
    "labeled_train, labeled_test = model_selection.train_test_split(\n",
    "    labeled_data, test_size=test_ratio, stratify=labeled_data)\n",
    "\n",
    "\n",
    "print(\"Train: \", len(labeled_train))\n",
    "print(\"Test: \", len(labeled_test))\n",
    "print(\"\\nTotal: \", len(labeled_train)+len(labeled_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difinição da função de custo Focal Loss\n",
    "\n",
    "import dill\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "     \n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        # y_pred = y_pred + epsilon\n",
    "        # Clip the prediciton value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = K.ones_like(y_true) * alpha\n",
    "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -K.log(p_t)\n",
    "        weight = alpha_t * K.pow((1 - p_t), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.mean(K.sum(loss, axis=1))\n",
    "        return loss\n",
    "\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n"
     ]
    }
   ],
   "source": [
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "\n",
    "train_targets = target_encoding.fit_transform(labeled_train)\n",
    "test_targets = target_encoding.transform(labeled_test)\n",
    "\n",
    "\n",
    "\n",
    "generator = FullBatchNodeGenerator(G, method=\"gcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "params = {'layer_sizes':[[64, 64], [128, 128], [256, 256]],\n",
    "        'activations': [[\"ReLU\", \"ReLU\"], [\"PReLU\", \"PReLU\"]],\n",
    "        'dropout':[0.01, 0.001, 0.1],\n",
    "        'learning_rate':[0.001, 0.0001, 0.00001],\n",
    "        'epochs':[300, 700],\n",
    "        'gamma':[0, 0.1, 0.2, 0.5, 1, 2, 5],\n",
    "        'alpha':[0.95, 0.90, 0.85, 0.80, 0.75, 0.50, 0.25]\n",
    "        }\n",
    "\n",
    "\n",
    "num_of_settings = len(list(ParameterGrid(params)))\n",
    "print (\"Grid Search: Trying {} different parameter settings...\".format(num_of_settings))\n",
    "param_num = 0\n",
    "\n",
    "for param_set in list(ParameterGrid(params)):\n",
    "\n",
    "  num_folds = 5\n",
    "\n",
    "  auc_pr_per_fold = []\n",
    "  loss_per_fold = []\n",
    "\n",
    "  kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "  fold_no = 1\n",
    "  for train, test in kfold.split(labeled_train.index, train_targets):\n",
    "\n",
    "    train_gen = generator.flow(labeled_train.index[train], train_targets[train])\n",
    "\n",
    "    gcn = GCN(\n",
    "      layer_sizes=param_set['layer_sizes'], activations=param_set['activations'], generator=generator, dropout=param_set['dropout']\n",
    "    )\n",
    "\n",
    "    x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "    predictions = layers.Dense(units=train_targets.shape[1], activation=\"sigmoid\")(x_out)\n",
    "\n",
    "    model = Model(inputs=x_inp, outputs=predictions)\n",
    "    model.compile(\n",
    "      optimizer=optimizers.Adam(learning_rate=param_set['learning_rate']),\n",
    "      #loss=losses.binary_crossentropy,\n",
    "      loss=[binary_focal_loss(gamma=param_set['gamma'], alpha=param_set['alpha'])],\n",
    "      metrics=[\"acc\", metrics.AUC(curve=\"ROC\", name=\"auc_roc\"), metrics.AUC(curve=\"PR\", name=\"auc_pr\")],\n",
    "    )\n",
    "\n",
    "    val_gen = generator.flow(labeled_train.index[test], train_targets[test])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    history = model.fit(\n",
    "      train_gen,\n",
    "      epochs=param_set['epochs'],\n",
    "      validation_data=val_gen,\n",
    "      verbose=2,\n",
    "      shuffle=False,\n",
    "    )\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    #val_gen = generator.flow(labeled_train.index[test], train_targets[test])\n",
    "    scores = model.evaluate(val_gen, verbose=0)\n",
    "    print('\\n')\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[3]} of {scores[3]*100}%')\n",
    "    auc_pr_per_fold.append(scores[3] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Save history to csv\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "    hist_csv_file = f'fold{fold_no}_hprd_param{param_num}.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "      hist_df.to_csv(f)\n",
    "    f.close()\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "  # Provide average scores\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print('Score per fold')\n",
    "  for i in range(0, len(auc_pr_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - AUC PR: {auc_pr_per_fold[i]}%')\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print('Average scores for all folds:')\n",
    "  print(f'> AUC PR: {np.mean(auc_pr_per_fold)} (+- {np.std(auc_pr_per_fold)})')\n",
    "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "  print('------------------------------------------------------------------------')\n",
    "\n",
    "  param_num += 1\n",
    "#print (\"[{} out of {} combinations]: {}\".format(param_num, num_of_settings, param_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: Trying 5292 different parameter settings...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'RandomizedSearchCV' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c6ba500cc892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mparam_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mparam_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mgcn\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mparam_distributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_sizes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'RandomizedSearchCV' object is not iterable"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'layer_sizes':[[64, 64], [128, 128], [256, 256]],\n",
    "        'activations': [[\"ReLU\", \"ReLU\"], [\"PReLU\", \"PReLU\"]],\n",
    "        'dropout':[0.01, 0.001, 0.1],\n",
    "        'learning_rate':[0.001, 0.0001, 0.00001],\n",
    "        'epochs':[300, 700],\n",
    "        'gamma':[0, 0.1, 0.2, 0.5, 1, 2, 5],\n",
    "        'alpha':[0.95, 0.90, 0.85, 0.80, 0.75, 0.50, 0.25]\n",
    "        }\n",
    "\n",
    "\n",
    "num_of_settings = len(list(ParameterGrid(param_grid)))\n",
    "print (\"Grid Search: Trying {} different parameter settings...\".format(num_of_settings))\n",
    "param_num = 0\n",
    "\n",
    "for param_set in list(RandomizedSearchCV(estimator= gcn,  param_distributions = param_grid)):\n",
    "        print(param_set['layer_sizes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ReLU', 'ReLU'], ['PReLU', 'PReLU']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['layer_sizes']\n",
    "\n",
    "params['activations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 237ms/step - loss: 0.2463 - acc: 0.9123 - auc_roc: 0.8162 - auc_pr: 0.4019\n",
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 0.2463\n",
      "\tacc: 0.9123\n",
      "\tauc_roc: 0.8162\n",
      "\tauc_pr: 0.4019\n"
     ]
    }
   ],
   "source": [
    "# Execução para os dados de teste\n",
    "\n",
    "test_gen = generator.flow(labeled_test.index, test_targets)\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = series_classes.index\n",
    "all_gen = generator.flow(all_nodes)\n",
    "all_predictions = model.predict(all_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0    8486\n",
      "-1.0    4633\n",
      " 1.0     868\n",
      "Name: True, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    13597\n",
       "1.0      390\n",
       "Name: Predicted, dtype: int64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_predictions = target_encoding.inverse_transform(all_predictions.squeeze())\n",
    "df = pd.DataFrame({\"Predicted\": node_predictions, \"True\": series_classes})\n",
    "\n",
    "#df.head(20)\n",
    "\n",
    "print(df['True'].value_counts(), \"\\n\")\n",
    "df['Predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar a variável all_predictions e a node_predictions para comparar\n",
    "# Usar a all_predictions para gerar gráficos da fig3 do artigo, sobre distribuição de nós \n",
    "\n",
    "df_predictions = pd.DataFrame({\"percent\": all_predictions.squeeze(), \"binary\": node_predictions, \"true\": series_classes})\n",
    "\n",
    "# Save to csv\n",
    "pred_csv_file = f'predictions_hprd_6a.csv'\n",
    "with open(pred_csv_file, mode='w') as f:\n",
    "    df_predictions.to_csv(f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "377d7a386a44a88aae8c34a08bf9c5a42083beb2aa1718a5ef987e2fcb7e0f84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
